{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "04. Building a \"fake news\" classifier.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMfnc8jG7Os13EtnAil0zjJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tanaymukherjee/Natural-Language-Processing/blob/master/04_Building_a_%22fake_news%22_classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mrsOVKgfNgd_",
        "colab_type": "text"
      },
      "source": [
        "# Building a \"fake news\" classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EdR-Bv1LGUOW",
        "colab_type": "text"
      },
      "source": [
        "## Read the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wdSaZs9EGK-m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import necessary libraries and packages\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# set the plots to display in the Jupyter notebook\n",
        "% matplotlib inline"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I6ghbeLugBnE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "outputId": "78fe6a66-315f-4bc2-84fd-09e2291627eb"
      },
      "source": [
        "# Mounting google colab with personal GDrive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dmjzK1BeF3mw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "outputId": "2fe435af-3061-4aa1-8042-eed6978d94f9"
      },
      "source": [
        "# Checking the content in my GDrive witihin the working directory\n",
        "\n",
        "!ls -ahl \"/content/gdrive/My Drive/Colab Notebooks/Data\""
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 32M\n",
            "-rw------- 1 root root  30M Jul 18 04:35 fake_or_real_news.csv\n",
            "-rw------- 1 root root 291K May 31 02:02 Index2018.csv\n",
            "-rw------- 1 root root 164K May 31 02:02 RandWalk.csv\n",
            "-rw------- 1 root root 492K Jun 16 22:51 spam.csv\n",
            "-rw------- 1 root root 1.1M Jun 16 03:45 tweets.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vM5QlBToF_jp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        },
        "outputId": "62557892-3237-4cdc-c9a1-7cb53a14cb06"
      },
      "source": [
        "# Read the csv file\n",
        "\n",
        "df = pd.read_csv(\"/content/gdrive/My Drive/Colab Notebooks/Data/fake_or_real_news.csv\")\n",
        "\n",
        "df.head()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>title</th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>8476</td>\n",
              "      <td>You Can Smell Hillary’s Fear</td>\n",
              "      <td>Daniel Greenfield, a Shillman Journalism Fello...</td>\n",
              "      <td>FAKE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>10294</td>\n",
              "      <td>Watch The Exact Moment Paul Ryan Committed Pol...</td>\n",
              "      <td>Google Pinterest Digg Linkedin Reddit Stumbleu...</td>\n",
              "      <td>FAKE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3608</td>\n",
              "      <td>Kerry to go to Paris in gesture of sympathy</td>\n",
              "      <td>U.S. Secretary of State John F. Kerry said Mon...</td>\n",
              "      <td>REAL</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>10142</td>\n",
              "      <td>Bernie supporters on Twitter erupt in anger ag...</td>\n",
              "      <td>— Kaydee King (@KaydeeKing) November 9, 2016 T...</td>\n",
              "      <td>FAKE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>875</td>\n",
              "      <td>The Battle of New York: Why This Primary Matters</td>\n",
              "      <td>It's primary day in New York and front-runners...</td>\n",
              "      <td>REAL</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0  ... label\n",
              "0        8476  ...  FAKE\n",
              "1       10294  ...  FAKE\n",
              "2        3608  ...  REAL\n",
              "3       10142  ...  FAKE\n",
              "4         875  ...  REAL\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2pCCikWeOoZ2",
        "colab_type": "text"
      },
      "source": [
        "## CountVectorizer for text classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xZlyfCRmGvGT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import the necessary modules\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l77wv_acGYCH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "outputId": "0023201a-c464-4155-c70b-988ca778657e"
      },
      "source": [
        "# Print the head of df\n",
        "print(df.head())\n",
        "\n",
        "# Create a series to store the labels: y\n",
        "y = df.label\n",
        "\n",
        "# Create training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(df['text'], y, test_size=0.33, random_state=53)\n",
        "\n",
        "# Initialize a CountVectorizer object: count_vectorizer\n",
        "count_vectorizer = CountVectorizer(stop_words='english')\n",
        "\n",
        "# Transform the training data using only the 'text' column values: count_train \n",
        "count_train = count_vectorizer.fit_transform(X_train)\n",
        "\n",
        "# Transform the test data using only the 'text' column values: count_test \n",
        "count_test = count_vectorizer.transform(X_test)\n",
        "\n",
        "# Print the first 10 features of the count_vectorizer\n",
        "print(count_vectorizer.get_feature_names()[:10])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   Unnamed: 0  ... label\n",
            "0        8476  ...  FAKE\n",
            "1       10294  ...  FAKE\n",
            "2        3608  ...  REAL\n",
            "3       10142  ...  FAKE\n",
            "4         875  ...  REAL\n",
            "\n",
            "[5 rows x 4 columns]\n",
            "['00', '000', '0000', '00000031', '000035', '00006', '0001', '0001pt', '000ft', '000km']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oRjGkc59HXg1",
        "colab_type": "text"
      },
      "source": [
        "## TfidfVectorizer for text classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dhHelu3dHdC-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4YLQFn0-HWHp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "outputId": "ffc81602-899e-455a-ad6e-7cff50c593c1"
      },
      "source": [
        "# Initialize a TfidfVectorizer object: tfidf_vectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_df=0.7)\n",
        "\n",
        "# Transform the training data: tfidf_train \n",
        "tfidf_train = tfidf_vectorizer.fit_transform(X_train)\n",
        "\n",
        "# Transform the test data: tfidf_test \n",
        "tfidf_test = tfidf_vectorizer.transform(X_test)\n",
        "\n",
        "# Print the first 10 features\n",
        "print(tfidf_vectorizer.get_feature_names()[:10])\n",
        "\n",
        "# Print the first 5 vectors of the tfidf training data\n",
        "print(tfidf_train.A[:5])\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['00', '000', '0000', '00000031', '000035', '00006', '0001', '0001pt', '000ft', '000km']\n",
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k2n1Jw7WHxOD",
        "colab_type": "text"
      },
      "source": [
        "## Inspecting the vectors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AfLQXbD1Hy7V",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 334
        },
        "outputId": "911621be-1be3-4333-b920-c24bd44558d5"
      },
      "source": [
        "# Converting them into pandas DataFrames\n",
        "\n",
        "# Create the CountVectorizer DataFrame: count_df\n",
        "count_df = pd.DataFrame(count_train.A, columns=count_vectorizer.get_feature_names())\n",
        "\n",
        "# Create the TfidfVectorizer DataFrame: tfidf_df\n",
        "tfidf_df = pd.DataFrame(tfidf_train.A, columns=tfidf_vectorizer.get_feature_names())\n",
        "\n",
        "# Print the head of count_df\n",
        "print(count_df.head())\n",
        "\n",
        "# Print the head of tfidf_df\n",
        "print(tfidf_df.head())\n",
        "\n",
        "# Calculate the difference in columns: difference\n",
        "difference = set(count_df.columns) - set(tfidf_df.columns)\n",
        "print(difference)\n",
        "\n",
        "# Check whether the DataFrames are equal\n",
        "print(count_df.equals(tfidf_df))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   00  000  0000  00000031  000035  00006  ...  ما  محاولات  من  هذا  والمرضى  ยงade\n",
            "0   0    0     0         0       0      0  ...   0        0   0    0        0      0\n",
            "1   0    0     0         0       0      0  ...   0        0   0    0        0      0\n",
            "2   0    0     0         0       0      0  ...   0        0   0    0        0      0\n",
            "3   0    0     0         0       0      0  ...   0        0   0    0        0      0\n",
            "4   0    0     0         0       0      0  ...   0        0   0    0        0      0\n",
            "\n",
            "[5 rows x 56922 columns]\n",
            "    00  000  0000  00000031  000035  ...  محاولات   من  هذا  والمرضى  ยงade\n",
            "0  0.0  0.0   0.0       0.0     0.0  ...      0.0  0.0  0.0      0.0    0.0\n",
            "1  0.0  0.0   0.0       0.0     0.0  ...      0.0  0.0  0.0      0.0    0.0\n",
            "2  0.0  0.0   0.0       0.0     0.0  ...      0.0  0.0  0.0      0.0    0.0\n",
            "3  0.0  0.0   0.0       0.0     0.0  ...      0.0  0.0  0.0      0.0    0.0\n",
            "4  0.0  0.0   0.0       0.0     0.0  ...      0.0  0.0  0.0      0.0    0.0\n",
            "\n",
            "[5 rows x 56922 columns]\n",
            "set()\n",
            "False\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GPRrqauLJO_H",
        "colab_type": "text"
      },
      "source": [
        "## Training and testing the \"fake news\" model with CountVectorizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_RDYg410JP_7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import the necessary modules\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn import metrics"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C_SIcfyMJSbO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "478a5276-e437-4136-a77a-373014c7f9f3"
      },
      "source": [
        "# Instantiate a Multinomial Naive Bayes classifier: nb_classifier\n",
        "nb_classifier = MultinomialNB()\n",
        "\n",
        "# Fit the classifier to the training data\n",
        "nb_classifier.fit(count_train, y_train)\n",
        "\n",
        "# Create the predicted tags: pred\n",
        "pred = nb_classifier.predict(count_test)\n",
        "\n",
        "# Calculate the accuracy score: score\n",
        "score = metrics.accuracy_score(y_test, pred)\n",
        "print(score)\n",
        "\n",
        "# Calculate the confusion matrix: cm\n",
        "cm = metrics.confusion_matrix(y_test, pred, labels=['FAKE', 'REAL'])\n",
        "print(cm)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.893352462936394\n",
            "[[ 865  143]\n",
            " [  80 1003]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7q0kS8C7JYfN",
        "colab_type": "text"
      },
      "source": [
        "## Training and testing the \"fake news\" model with TfidfVectorizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p3EvAECtJcbc",
        "colab_type": "text"
      },
      "source": [
        "**Steps:**\n",
        "1. Instantiate a MultinomialNB classifier called nb_classifier.\n",
        "2. Fit the classifier to the training data.\n",
        "3. Compute the predicted tags for the test data.\n",
        "4. Calculate and print the accuracy score of the classifier.\n",
        "5. Compute the confusion matrix. As in the previous exercise, specify the keyword argument labels=['FAKE', 'REAL'] so that the resulting confusion matrix is easier to read."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uLso9zyCJZI0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "e2d4a813-0056-4ffd-bf06-ccbe65022f36"
      },
      "source": [
        "# Create a Multinomial Naive Bayes classifier: nb_classifier\n",
        "nb_classifier = MultinomialNB()\n",
        "\n",
        "# Fit the classifier to the training data\n",
        "nb_classifier.fit(tfidf_train, y_train)\n",
        "\n",
        "# Create the predicted tags: pred\n",
        "pred = nb_classifier.predict(tfidf_test)\n",
        "\n",
        "# Calculate the accuracy score: score\n",
        "score = metrics.accuracy_score(y_test, pred)\n",
        "print(score)\n",
        "\n",
        "# Calculate the confusion matrix: cm\n",
        "cm = metrics.confusion_matrix(y_test, pred, labels=['FAKE', 'REAL'])\n",
        "print(cm)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.8565279770444764\n",
            "[[ 739  269]\n",
            " [  31 1052]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QaNH0sYcKfUv",
        "colab_type": "text"
      },
      "source": [
        "## Improving the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ggyfXyUKhfU",
        "colab_type": "text"
      },
      "source": [
        "**Steps:**\n",
        "\n",
        "1. Create a list of alphas to try using np.arange(). Values should range from 0 to 1 with steps of 0.1.\n",
        "2. Create a function train_and_predict() that takes in one argument: alpha. The function should:\n",
        "3. Instantiate a MultinomialNB classifier with alpha=alpha.\n",
        "4. Fit it to the training data.\n",
        "5. Compute predictions on the test data.\n",
        "6. Compute and return the accuracy score.\n",
        "7. Using a for loop, print the alpha, score and a newline in between. Use the train_and_predict() function to compute the score. Does the score change along with the alpha? What is the best alpha?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GPplYajlK0bB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 580
        },
        "outputId": "d0ba5a93-4aa8-4c51-92ec-8b11d03f3d5c"
      },
      "source": [
        "# Create the list of alphas: alphas\n",
        "alphas = np.arange(0, 1, .1)\n",
        "\n",
        "# Define train_and_predict()\n",
        "def train_and_predict(alpha):\n",
        "    # Instantiate the classifier: nb_classifier\n",
        "    nb_classifier = MultinomialNB(alpha=alpha)\n",
        "    # Fit to the training data\n",
        "    nb_classifier.fit(tfidf_train, y_train)\n",
        "    # Predict the labels: pred\n",
        "    pred = nb_classifier.predict(tfidf_test)\n",
        "    # Compute accuracy: score\n",
        "    score = metrics.accuracy_score(y_test, pred)\n",
        "    return score\n",
        "\n",
        "# Iterate over the alphas and print the corresponding score\n",
        "for alpha in alphas:\n",
        "    print('Alpha: ', alpha)\n",
        "    print('Score: ', train_and_predict(alpha))\n",
        "    print()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Alpha:  0.0\n",
            "Score:  0.8813964610234337\n",
            "\n",
            "Alpha:  0.1\n",
            "Score:  0.8976566236250598\n",
            "\n",
            "Alpha:  0.2\n",
            "Score:  0.8938307030129125\n",
            "\n",
            "Alpha:  0.30000000000000004\n",
            "Score:  0.8900047824007652\n",
            "\n",
            "Alpha:  0.4\n",
            "Score:  0.8857006217120995\n",
            "\n",
            "Alpha:  0.5\n",
            "Score:  0.8842659014825442\n",
            "\n",
            "Alpha:  0.6000000000000001\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/naive_bayes.py:507: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
            "  'setting alpha = %.1e' % _ALPHA_MIN)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Score:  0.874701099952176\n",
            "\n",
            "Alpha:  0.7000000000000001\n",
            "Score:  0.8703969392635102\n",
            "\n",
            "Alpha:  0.8\n",
            "Score:  0.8660927785748446\n",
            "\n",
            "Alpha:  0.9\n",
            "Score:  0.8589191774270684\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ObRi_6xWLGNV",
        "colab_type": "text"
      },
      "source": [
        "## Inspecting the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ju_9HA0ULLv8",
        "colab_type": "text"
      },
      "source": [
        "**Steps:**\n",
        "1. Save the class labels as class_labels by accessing the .classes_ attribute of nb_classifier.\n",
        "2. Extract the features using the .get_feature_names() method of tfidf_vectorizer.\n",
        "3. Create a zipped array of the classifier coefficients with the feature names and sort them by the coefficients. To do this, first use zip() with the arguments nb_classifier.coef_[0] and feature_names. Then, use sorted() on this.\n",
        "4. Print the top 20 weighted features for the first label of class_labels and print the bottom 20 weighted features for the second label of class_labels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mw-R0wuNLH-8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "5c357414-2c69-462f-c66b-877675425a5d"
      },
      "source": [
        "# Get the class labels: class_labels\n",
        "class_labels = nb_classifier.classes_\n",
        "\n",
        "# Extract the features: feature_names\n",
        "feature_names = tfidf_vectorizer.get_feature_names()\n",
        "\n",
        "# Zip the feature names together with the coefficient array and sort by weights: feat_with_weights\n",
        "feat_with_weights = sorted(zip(nb_classifier.coef_[0], feature_names))\n",
        "\n",
        "# Print the first class label and the top 20 feat_with_weights entries\n",
        "print(class_labels[0], feat_with_weights[:20])\n",
        "\n",
        "# Print the second class label and the bottom 20 feat_with_weights entries\n",
        "print(class_labels[1], feat_with_weights[-20:])"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "FAKE [(-11.316312804238807, '0000'), (-11.316312804238807, '000035'), (-11.316312804238807, '0001'), (-11.316312804238807, '0001pt'), (-11.316312804238807, '000km'), (-11.316312804238807, '0011'), (-11.316312804238807, '006s'), (-11.316312804238807, '007'), (-11.316312804238807, '007s'), (-11.316312804238807, '008s'), (-11.316312804238807, '0099'), (-11.316312804238807, '00am'), (-11.316312804238807, '00p'), (-11.316312804238807, '00pm'), (-11.316312804238807, '014'), (-11.316312804238807, '015'), (-11.316312804238807, '018'), (-11.316312804238807, '01am'), (-11.316312804238807, '020'), (-11.316312804238807, '023')]\n",
            "REAL [(-7.742481952533027, 'states'), (-7.717550034444668, 'rubio'), (-7.703583809227384, 'voters'), (-7.654774992495461, 'house'), (-7.649398936153309, 'republicans'), (-7.6246184189367, 'bush'), (-7.616556675728881, 'percent'), (-7.545789237823644, 'people'), (-7.516447881078008, 'new'), (-7.448027933291952, 'party'), (-7.411148410203476, 'cruz'), (-7.410910239085596, 'state'), (-7.35748985914622, 'republican'), (-7.33649923948987, 'campaign'), (-7.2854057032685775, 'president'), (-7.2166878130917755, 'sanders'), (-7.108263114902301, 'obama'), (-6.724771332488041, 'clinton'), (-6.5653954389926845, 'said'), (-6.328486029596207, 'trump')]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}