{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "09. Scaling and performance.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNDp4j+8wiYtMWrifF8Dr15",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tanaymukherjee/Natural-Language-Processing/blob/master/09_Scaling_and_performance.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xvPd38CJvSXi",
        "colab_type": "text"
      },
      "source": [
        "# Scaling and Performance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kjf6cGT1vVJ4",
        "colab_type": "text"
      },
      "source": [
        "## Processing streams"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oKCWvG17tSOk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import spacy"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EqpXKWZ-tU9z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create an NLP object\n",
        "from spacy.lang.en import English\n",
        "nlp = English()"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "niyy4CmztWL6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import the Doc class\n",
        "from spacy.tokens import Doc, Span, Token"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XCmessb3tWE6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Texts = ['McDonalds is my favorite restaurant.',\n",
        " 'Here I thought @McDonalds only had precooked burgers but it seems they only have not cooked ones?? I have no time to get sick..',\n",
        " 'People really still eat McDonalds :(',\n",
        " 'The McDonalds in Spain has chicken wings. My heart is so happy ',\n",
        " '@McDonalds Please bring back the most delicious fast food sandwich of all times!!....The Arch Deluxe :P',\n",
        " 'please hurry and open. I WANT A #McRib SANDWICH SO BAD! :D',\n",
        " 'This morning i made a terrible decision by gettin mcdonalds and now my stomach is payin for it']"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9yc0BjLyt73U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from spacy.matcher import Matcher\n",
        "matcher = Matcher(nlp.vocab)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mRfSeADUt8SA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "b1d2d2de-6e57-46ae-cf00-e5615ed27784"
      },
      "source": [
        "# Define the custom component\n",
        "def length_component(doc):\n",
        "    # Get the doc's length\n",
        "    doc_length = len(doc)\n",
        "    print(\"This document is {} tokens long.\".format(doc_length))\n",
        "    # Return the doc\n",
        "    return doc\n",
        "\n",
        "# Load the small English model\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Add the component first in the pipeline and print the pipe names\n",
        "nlp.add_pipe(length_component, first=True)\n",
        "print(nlp.pipe_names)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['length_component', 'tagger', 'parser', 'ner']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LdBWkbiEtjq4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 263
        },
        "outputId": "b76eb0fd-c93a-4e60-8199-a72260994940"
      },
      "source": [
        "# Process the texts and print the adjectives\n",
        "for text in Texts:\n",
        "    doc = nlp(text)\n",
        "    print([token.text for token in doc if token.pos_ == 'ADJ'])"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "This document is 6 tokens long.\n",
            "['favorite']\n",
            "This document is 27 tokens long.\n",
            "['sick']\n",
            "This document is 6 tokens long.\n",
            "[]\n",
            "This document is 13 tokens long.\n",
            "['happy']\n",
            "This document is 18 tokens long.\n",
            "['delicious', 'fast']\n",
            "This document is 15 tokens long.\n",
            "[]\n",
            "This document is 18 tokens long.\n",
            "['terrible']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G084WA1zv2D7",
        "colab_type": "text"
      },
      "source": [
        "Rewrite the example to use nlp.pipe. Instead of iterating over the texts and processing them, iterate over the doc objects yielded by nlp.pipe."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TR5PPkPRuWVS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 263
        },
        "outputId": "c63dfd76-94f7-4eb1-9b4d-6600e9253fe0"
      },
      "source": [
        "# Process the texts and print the adjectives\n",
        "for doc in nlp.pipe(Texts):\n",
        "    print([token.text for token in doc if token.pos_ == 'ADJ'])"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "This document is 6 tokens long.\n",
            "This document is 27 tokens long.\n",
            "This document is 6 tokens long.\n",
            "This document is 13 tokens long.\n",
            "This document is 18 tokens long.\n",
            "This document is 15 tokens long.\n",
            "This document is 18 tokens long.\n",
            "['favorite']\n",
            "['sick']\n",
            "[]\n",
            "['happy']\n",
            "['delicious', 'fast']\n",
            "[]\n",
            "['terrible']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FnEaFeTtv5WQ",
        "colab_type": "text"
      },
      "source": [
        "Rewrite the example to use nlp.pipe. Don't forget to call list() around the result to turn it into a list."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JqKsfpjmuctz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        },
        "outputId": "94981d30-8982-47fb-bc76-e6ffe464b4bf"
      },
      "source": [
        "# Process the texts and print the entities\n",
        "docs = list(nlp.pipe(Texts))\n",
        "entities = [doc.ents for doc in docs]\n",
        "print(*entities)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "This document is 6 tokens long.\n",
            "This document is 27 tokens long.\n",
            "This document is 6 tokens long.\n",
            "This document is 13 tokens long.\n",
            "This document is 18 tokens long.\n",
            "This document is 15 tokens long.\n",
            "This document is 18 tokens long.\n",
            "(McDonalds,) () (McDonalds,) (McDonalds, Spain) (The Arch Deluxe,) () (This morning, gettin mcdonalds)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SswXWcARv6D_",
        "colab_type": "text"
      },
      "source": [
        "Rewrite the example to use nlp.pipe. Don't forget to call list() around the result to turn it into a list."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yh1RkjdfuiVA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "3bc1464e-986c-4893-be69-f49621e5b6fc"
      },
      "source": [
        "people = ['David Bowie', 'Angela Merkel', 'Lady Gaga']\n",
        "\n",
        "# Create a list of patterns for the PhraseMatcher\n",
        "patterns = list(nlp.pipe(people))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "This document is 2 tokens long.\n",
            "This document is 2 tokens long.\n",
            "This document is 2 tokens long.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xvy4zXoXv-54",
        "colab_type": "text"
      },
      "source": [
        "## Processing data with context"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PbfX-0AQv_9R",
        "colab_type": "text"
      },
      "source": [
        "A list of (text, context) examples is available as the variable Data. The texts are quotes from famous books, and the contexts dictionaries with the keys 'author' and 'book'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hHrzcujGu-7L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Data = [('One morning, when Gregor Samsa woke from troubled dreams, he found himself transformed in his bed into a horrible vermin.',\n",
        "  {'author': 'Franz Kafka', 'book': 'Metamorphosis'}),\n",
        " (\"I know not all that may be coming, but be it what it will, I'll go to it laughing.\",\n",
        "  {'author': 'Herman Melville', 'book': 'Moby-Dick or, The Whale'}),\n",
        " ('It was the best of times, it was the worst of times.',\n",
        "  {'author': 'Charles Dickens', 'book': 'A Tale of Two Cities'}),\n",
        " ('The only people for me are the mad ones, the ones who are mad to live, mad to talk, mad to be saved, desirous of everything at the same time, the ones who never yawn or say a commonplace thing, but burn, burn, burn like fabulous yellow roman candles exploding like spiders across the stars.',\n",
        "  {'author': 'Jack Kerouac', 'book': 'On the Road'}),\n",
        " ('It was a bright cold day in April, and the clocks were striking thirteen.',\n",
        "  {'author': 'George Orwell', 'book': '1984'}),\n",
        " ('Nowadays people know the price of everything and the value of nothing.',\n",
        "  {'author': 'Oscar Wilde', 'book': 'The Picture Of Dorian Gray'})]"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rxOnpbY_wGLg",
        "colab_type": "text"
      },
      "source": [
        "Import the Doc class and use the set_extension method to register the custom attributes 'author' and 'book', which default to None."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pGPQ8knguuxi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import the Doc class\n",
        "from spacy.tokens import Doc\n",
        "\n",
        "# Register the Doc extension 'author' (default None)\n",
        "Doc.set_extension('author', default=None)\n",
        "\n",
        "# Register the Doc extension 'book' (default None)\n",
        "Doc.set_extension('book', default=None)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OsUvKFdUwPsx",
        "colab_type": "text"
      },
      "source": [
        "1. Process the (text, context) tuples in Data using nlp.pipe with as_tuples=True.\n",
        "2. Overwrite the doc._.book and doc._.author with the respective info passed in as the context."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pUzVFq41uyNy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        },
        "outputId": "d8571102-43eb-4b83-9b9b-864f6766d7a4"
      },
      "source": [
        "# Import the Doc class and register the extensions 'author' and 'book'\n",
        "from spacy.tokens import Doc\n",
        "Doc.set_extension('book', default=None, force=True)\n",
        "Doc.set_extension('author', default=None, force=True)\n",
        "\n",
        "for doc, context in nlp.pipe(Data, as_tuples=True):\n",
        "    # Set the doc._.book and doc._.author attributes from the context\n",
        "    doc._.book = context['book']\n",
        "    doc._.author = context['author']\n",
        "    \n",
        "    # Print the text and custom attribute data\n",
        "    print(doc.text, '\\n', \"— '{}' by {}\".format(doc._.book, doc._.author), '\\n')"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "This document is 23 tokens long.\n",
            "This document is 23 tokens long.\n",
            "This document is 14 tokens long.\n",
            "This document is 64 tokens long.\n",
            "This document is 16 tokens long.\n",
            "This document is 13 tokens long.\n",
            "One morning, when Gregor Samsa woke from troubled dreams, he found himself transformed in his bed into a horrible vermin. \n",
            " — 'Metamorphosis' by Franz Kafka \n",
            "\n",
            "I know not all that may be coming, but be it what it will, I'll go to it laughing. \n",
            " — 'Moby-Dick or, The Whale' by Herman Melville \n",
            "\n",
            "It was the best of times, it was the worst of times. \n",
            " — 'A Tale of Two Cities' by Charles Dickens \n",
            "\n",
            "The only people for me are the mad ones, the ones who are mad to live, mad to talk, mad to be saved, desirous of everything at the same time, the ones who never yawn or say a commonplace thing, but burn, burn, burn like fabulous yellow roman candles exploding like spiders across the stars. \n",
            " — 'On the Road' by Jack Kerouac \n",
            "\n",
            "It was a bright cold day in April, and the clocks were striking thirteen. \n",
            " — '1984' by George Orwell \n",
            "\n",
            "Nowadays people know the price of everything and the value of nothing. \n",
            " — 'The Picture Of Dorian Gray' by Oscar Wilde \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ln_sfI7-wWNr",
        "colab_type": "text"
      },
      "source": [
        "## Selective processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FmvvCYCNwcQR",
        "colab_type": "text"
      },
      "source": [
        "In this exercise, we will use the nlp.make_doc and nlp.disable_pipes methods to only run selected components when processing a text. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZaLAJiIEwqOY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "outputId": "aebbf929-2242-4c4a-f48c-a8e617a96d90"
      },
      "source": [
        "!python -m spacy download en_core_web_sm"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: en_core_web_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz#egg=en_core_web_sm==2.2.5 in /usr/local/lib/python3.6/dist-packages (2.2.5)\n",
            "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (49.1.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.3)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.18.5)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.7.0)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.2)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.7.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.1.0)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U4b7mIodwgji",
        "colab_type": "text"
      },
      "source": [
        "Rewrite the code to only tokenize the text using nlp.make_doc."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j2YfXcLovIUn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "a53fd684-f8c2-423f-86f8-ebaf52d228d0"
      },
      "source": [
        "text = \"Chick-fil-A is an American fast food restaurant chain headquartered in the city of College Park, Georgia, specializing in chicken sandwiches.\"\n",
        "\n",
        "# Only tokenize the text\n",
        "doc = nlp.make_doc(text)\n",
        "\n",
        "print([token.text for token in doc])"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Chick', '-', 'fil', '-', 'A', 'is', 'an', 'American', 'fast', 'food', 'restaurant', 'chain', 'headquartered', 'in', 'the', 'city', 'of', 'College', 'Park', ',', 'Georgia', ',', 'specializing', 'in', 'chicken', 'sandwiches', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YfYnjczOwiwf",
        "colab_type": "text"
      },
      "source": [
        "1. Disable the tagger and parser using the nlp.disable_pipes method.\n",
        "2. Process the text and print all entities in the doc."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QGdWtpvbvKNX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "5facf55b-1a74-4682-816d-ef9b433e114b"
      },
      "source": [
        "text = \"Chick-fil-A is an American fast food restaurant chain headquartered in the city of College Park, Georgia, specializing in chicken sandwiches.\"\n",
        "\n",
        "# Disable the tagger and parser\n",
        "with nlp.disable_pipes('tagger', 'parser'):\n",
        "    # Process the text\n",
        "    doc = nlp(text)\n",
        "    # Print the entities in the doc\n",
        "    print(doc.ents)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "This document is 27 tokens long.\n",
            "(American, College Park, Georgia)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}